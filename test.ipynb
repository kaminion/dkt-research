{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import random \n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn import metrics \n",
    "\n",
    "from data_loaders.assist2009 import ASSIST2009\n",
    "from data_loaders.assist2012 import ASSIST2012\n",
    "from data_loaders.ednet01 import EdNet01\n",
    "\n",
    "from models.dkvmn_text import SUBJ_DKVMN\n",
    "from models.dkvmn_text import train_model as plus_train\n",
    "\n",
    "from models.utils import collate_fn, collate_ednet, cal_acc_class\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 1004\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dkvmn+'\n",
    "dataset_name = 'ASSIST2009'\n",
    "dataset = None\n",
    "ckpts = f\"ckpts/{model_name}/{dataset_name}/\"\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "    model_config = config[model_name]\n",
    "    train_config = config[\"train_config\"]\n",
    "    \n",
    "batch_size = train_config[\"batch_size\"]\n",
    "num_epochs = train_config[\"num_epochs\"]\n",
    "train_ratio = train_config[\"train_ratio\"]\n",
    "learning_rate = train_config[\"learning_rate\"]\n",
    "optimizer = train_config[\"optimizer\"] # can be sgd, adam\n",
    "seq_len = train_config[\"seq_len\"] # 샘플링 할 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 추가 가능\n",
    "collate_pt = collate_fn\n",
    "if dataset_name == \"ASSIST2009\":\n",
    "    dataset = ASSIST2009(seq_len, 'datasets/ASSIST2009/')\n",
    "elif dataset_name == \"ASSIST2012\":\n",
    "    dataset = ASSIST2012(seq_len, 'datasets/ASSIST2012/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, test_loader, ckpt_path):\n",
    "    '''\n",
    "        Args:\n",
    "            train_loader: the PyTorch DataLoader instance for training\n",
    "            test_loader: the PyTorch DataLoader instance for test\n",
    "            num_epochs: the number of epochs\n",
    "            opt: the optimization to train this model\n",
    "            ckpt_path: the path to save this model's parameters\n",
    "    '''\n",
    "    aucs = []\n",
    "    loss_means = []  \n",
    "    accs = []\n",
    "    q_accs = {}\n",
    "    \n",
    "    max_auc = 0\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load(os.path.join(ckpt_path, \"model.ckpt\"), map_location=device))\n",
    "    loss_mean = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            q, r, qshft_seqs, rshft_seqs, m, bert_s, bert_t, bert_m, q2diff_seqs, pid_seqs, pidshift, hint_seqs = data\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            y, Mv, w = model(q.long(), r.long(), bert_s, bert_t, bert_m, q2diff_seqs.long(), pid_seqs.long())\n",
    "\n",
    "            # y와 t 변수에 있는 행렬들에서 마스킹이 true로 된 값들만 불러옴\n",
    "            q = torch.masked_select(q, m).detach().cpu()\n",
    "            y = torch.masked_select(y, m).detach().cpu()\n",
    "            t = torch.masked_select(r, m).detach().cpu()\n",
    "\n",
    "            auc = metrics.roc_auc_score(\n",
    "                y_true=t.numpy(), y_score=y.numpy()\n",
    "            )\n",
    "            bin_y = [1 if p >= 0.5 else 0 for p in y.numpy()]\n",
    "            acc = metrics.accuracy_score(t.numpy(), bin_y)\n",
    "            loss = binary_cross_entropy(y, t) # 실제 y^T와 원핫 결합, 다음 answer 간 cross entropy\n",
    "\n",
    "            print(f\"[Test] number: {i}, AUC: {auc}, ACC: :{acc} Loss: {loss} \")\n",
    "\n",
    "            # evaluation metrics\n",
    "            aucs.append(auc)\n",
    "            loss_mean.append(loss)     \n",
    "            accs.append(acc)\n",
    "            q_accs, cnt = cal_acc_class(q.long(), t.long(), bin_y)\n",
    "            break\n",
    "        loss_means.append(np.mean(loss_mean))\n",
    "\n",
    "\n",
    "    return aucs, loss_means, accs, q_accs, cnt, Mv, w, q, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, **model_config)).to(device)\n",
    "train_model = train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할\n",
    "data_size = len(dataset)\n",
    "train_size = int(data_size * train_ratio) \n",
    "valid_size = int(data_size * ((1.0 - train_ratio) / 2.0))\n",
    "test_size = data_size - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, valid_size, test_size], generator=torch.Generator(device=device)\n",
    ")\n",
    "\n",
    "# pickle에 얼마만큼 분할했는지 읽기\n",
    "if os.path.exists(os.path.join(dataset.dataset_dir, \"train_indices.pkl\")):\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"train_indices.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        train_dataset.indices = pickle.load(f)\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"valid_indicies.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        valid_dataset.indices = pickle.load(f)\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"test_indices.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        test_dataset.indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] number: 0, AUC: 0.9681324808848354, ACC: :0.9289991445680068 Loss: 0.18885724246501923 \n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "\n",
    "if optimizer == \"sgd\":\n",
    "    opt = SGD(model.parameters(), learning_rate, momentum=0.9)\n",
    "elif optimizer == \"adam\":\n",
    "    opt = Adam(model.parameters(), learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.5)\n",
    "opt.lr_scheduler = lr_scheduler\n",
    "\n",
    "# 모델에서 미리 정의한 함수로 AUCS와 LOSS 계산    \n",
    "aucs, loss_means, accs, q_accs, q_cnts, Mv, w, q, t = \\\n",
    "    train_model(\n",
    "        model, test_loader, ckpts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.8469)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(Mv[:, :-1])) # 컨셉 수 / 시퀀스? 임베딩 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   0.,   0.,  ..., 100., 100.,  69.]) tensor([0., 0., 1.,  ..., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(q, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([102.])\n"
     ]
    }
   ],
   "source": [
    "print(q[q==102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 100, 110, 100]) torch.Size([24, 100, 110])\n",
      "torch.Size([24, 100, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-f6b010e24dca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# print(.k_emb_layer(q))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSUBJ_DKVMN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_qid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_pid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 4"
     ]
    }
   ],
   "source": [
    "# 모든 W값에 대한 빈 텐서를 생성\n",
    "# 마지막 layer 빼서 넘겨보기\n",
    "\n",
    "# Read Process = w * M\n",
    "# 그래서 R 값은 마스크한 형태이고, W값은 마스킹 한 쪽에 0값 달라는 얘기인 것 같음\n",
    "# 그럼 결론적으로 메모리만 들어가면 됨\n",
    "# input embedding 의 weight값에 마스킹 -> 문제 정보 이그노어\n",
    "# 1. R 안에 들어가는 Corrleation 값 마스킹 (컨셉별로)\n",
    "# 2. Input Embedding (key embedding) 값 마스킹 (컨셉별로)\n",
    "# 3. Linear 레이어에 통과 => 이게 concept state가 될거라고 함..\n",
    "# Memory \n",
    "\n",
    "model = SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, **model_config)\n",
    "knowledge_state = []\n",
    "\n",
    "# 1. 외부메모리 컨셉 별 Corrleation 값 마스킹\n",
    "m = torch.zeros_like(w).unsqueeze(-1)\n",
    "print(m.shape)\n",
    "\n",
    "# 2. 컨셉별 Key Embedding 값 마스킹\n",
    "model.f_layer()\n",
    "\n",
    "# 3. Linear 레이어에 통과\n",
    "f = torch.tanh(\n",
    "    model.f_layer(\n",
    "    torch.cat(\n",
    "        [\n",
    "            (w.unsqueeze(-1) * Mv[:, :-1]).sum(-2),\n",
    "            k + diff\n",
    "        ],\n",
    "        dim=-1\n",
    "    )\n",
    "    )\n",
    ")\n",
    "\n",
    "a = torch.zeros_like(w)\n",
    "print(Mv[:, :-1].shape, a.shape)\n",
    "a[12][99][4] = 1\n",
    "print(torch.concat([a.unsqueeze(-1), Mv[:, :-1]], dim=-1).sum(-2).shape)\n",
    "# print(.k_emb_layer(q))\n",
    "print(SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, **model_config).f_layer((torch.concat([a, torch.zeros_like(Mv[:, :-1])], dim=-1).sum(-2).T * torch.concat([a.unsqueeze(-1), Mv[:, :-1]], dim=-1).sum(-2))))\n",
    "\n",
    "\n",
    "print(w[0][99][4])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
