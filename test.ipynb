{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user2\\Desktop\\master_degree\\research\\env_research\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import random \n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn import metrics \n",
    "\n",
    "from data_loaders.assist2009 import ASSIST2009\n",
    "from data_loaders.assist2012 import ASSIST2012\n",
    "from data_loaders.ednet01 import EdNet01\n",
    "\n",
    "from models.dkvmn_text import SUBJ_DKVMN\n",
    "from models.dkvmn_text import train_model as plus_train\n",
    "\n",
    "from models.utils import collate_fn, collate_ednet, cal_acc_class\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 1004\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dkvmn+'\n",
    "dataset_name = 'ASSIST2009'\n",
    "dataset = None\n",
    "ckpts = f\"ckpts/{model_name}/{dataset_name}/\"\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "    model_config = config[model_name]\n",
    "    train_config = config[\"train_config\"]\n",
    "    \n",
    "batch_size = train_config[\"batch_size\"]\n",
    "num_epochs = train_config[\"num_epochs\"]\n",
    "train_ratio = train_config[\"train_ratio\"]\n",
    "learning_rate = train_config[\"learning_rate\"]\n",
    "optimizer = train_config[\"optimizer\"] # can be sgd, adam\n",
    "seq_len = train_config[\"seq_len\"] # 샘플링 할 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 추가 가능\n",
    "collate_pt = collate_fn\n",
    "if dataset_name == \"ASSIST2009\":\n",
    "    dataset = ASSIST2009(seq_len, 'datasets/ASSIST2009/')\n",
    "elif dataset_name == \"ASSIST2012\":\n",
    "    dataset = ASSIST2012(seq_len, 'datasets/ASSIST2012/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, test_loader, ckpt_path):\n",
    "    '''\n",
    "        Args:\n",
    "            train_loader: the PyTorch DataLoader instance for training\n",
    "            test_loader: the PyTorch DataLoader instance for test\n",
    "            num_epochs: the number of epochs\n",
    "            opt: the optimization to train this model\n",
    "            ckpt_path: the path to save this model's parameters\n",
    "    '''\n",
    "    aucs = []\n",
    "    loss_means = []  \n",
    "    accs = []\n",
    "    q_accs = {}\n",
    "    \n",
    "    max_auc = 0\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load(os.path.join(ckpt_path, \"model.ckpt\"), map_location=device))\n",
    "    loss_mean = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            q, r, qshft_seqs, rshft_seqs, m, bert_s, bert_t, bert_m, q2diff_seqs, pid_seqs, pidshift, hint_seqs = data\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            y, Mv, w = model(q.long(), r.long(), bert_s, bert_t, bert_m, q2diff_seqs.long(), pid_seqs.long())\n",
    "\n",
    "            # y와 t 변수에 있는 행렬들에서 마스킹이 true로 된 값들만 불러옴\n",
    "            q = torch.masked_select(q, m).detach().cpu()\n",
    "            y = torch.masked_select(y, m).detach().cpu()\n",
    "            t = torch.masked_select(r, m).detach().cpu()\n",
    "\n",
    "            auc = metrics.roc_auc_score(\n",
    "                y_true=t.numpy(), y_score=y.numpy()\n",
    "            )\n",
    "            bin_y = [1 if p >= 0.5 else 0 for p in y.numpy()]\n",
    "            acc = metrics.accuracy_score(t.numpy(), bin_y)\n",
    "            loss = binary_cross_entropy(y, t) # 실제 y^T와 원핫 결합, 다음 answer 간 cross entropy\n",
    "\n",
    "            print(f\"[Test] number: {i}, AUC: {auc}, ACC: :{acc} Loss: {loss} \")\n",
    "\n",
    "            # evaluation metrics\n",
    "            aucs.append(auc)\n",
    "            loss_mean.append(loss)     \n",
    "            accs.append(acc)\n",
    "            q_accs, cnt = cal_acc_class(q.long(), t.long(), bin_y)\n",
    "        loss_means.append(np.mean(loss_mean))\n",
    "\n",
    "\n",
    "    return aucs, loss_means, accs, q_accs, cnt, Mv, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, **model_config)).to(device)\n",
    "train_model = train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할\n",
    "data_size = len(dataset)\n",
    "train_size = int(data_size * train_ratio) \n",
    "valid_size = int(data_size * ((1.0 - train_ratio) / 2.0))\n",
    "test_size = data_size - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, valid_size, test_size], generator=torch.Generator(device=device)\n",
    ")\n",
    "\n",
    "# pickle에 얼마만큼 분할했는지 읽기\n",
    "if os.path.exists(os.path.join(dataset.dataset_dir, \"train_indices.pkl\")):\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"train_indices.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        train_dataset.indices = pickle.load(f)\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"valid_indicies.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        valid_dataset.indices = pickle.load(f)\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"test_indices.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        test_dataset.indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] number: 0, AUC: 0.9837072343545911, ACC: :0.9469632164242943 Loss: 0.1328725516796112 \n",
      "[Test] number: 1, AUC: 0.9822715815499222, ACC: :0.9587878787878787 Loss: 0.139503613114357 \n",
      "[Test] number: 2, AUC: 0.9934436913451512, ACC: :0.9694767441860465 Loss: 0.07316002994775772 \n",
      "[Test] number: 3, AUC: 0.9889503043888026, ACC: :0.9680851063829787 Loss: 0.08745691925287247 \n",
      "[Test] number: 4, AUC: 0.9846775343177949, ACC: :0.9506976744186046 Loss: 0.1377621591091156 \n",
      "[Test] number: 5, AUC: 0.9635718083086504, ACC: :0.9308510638297872 Loss: 0.22788003087043762 \n",
      "[Test] number: 6, AUC: 0.9745466553811428, ACC: :0.9473684210526315 Loss: 0.17726880311965942 \n",
      "[Test] number: 7, AUC: 0.9612836438923396, ACC: :0.9347133757961783 Loss: 0.20352251827716827 \n",
      "[Test] number: 8, AUC: 0.9748021139727862, ACC: :0.9364326375711575 Loss: 0.17721284925937653 \n",
      "[Test] number: 9, AUC: 0.9704872220444727, ACC: :0.9231916480238628 Loss: 0.22785037755966187 \n",
      "[Test] number: 10, AUC: 0.9896079377818652, ACC: :0.9631268436578171 Loss: 0.10459514707326889 \n",
      "[Test] number: 11, AUC: 0.9756833685037051, ACC: :0.9449244060475162 Loss: 0.1736498326063156 \n",
      "[Test] number: 12, AUC: 0.9692824311129105, ACC: :0.9284239504473503 Loss: 0.2130720466375351 \n",
      "[Test] number: 13, AUC: 0.9908297520661157, ACC: :0.9688581314878892 Loss: 0.09046026319265366 \n",
      "[Test] number: 14, AUC: 0.9889449084209035, ACC: :0.9608108108108108 Loss: 0.11515052616596222 \n",
      "[Test] number: 15, AUC: 0.97459326656322, ACC: :0.940848990953375 Loss: 0.18158672749996185 \n",
      "[Test] number: 16, AUC: 0.9722238100472684, ACC: :0.9482758620689655 Loss: 0.17475394904613495 \n",
      "[Test] number: 17, AUC: 0.9814841299839615, ACC: :0.9479522184300341 Loss: 0.1504034548997879 \n",
      "[Test] number: 18, AUC: 0.9773843548216364, ACC: :0.9516957862281603 Loss: 0.16273233294487 \n",
      "[Test] number: 19, AUC: 0.9795756898120074, ACC: :0.9512195121951219 Loss: 0.1457514613866806 \n",
      "[Test] number: 20, AUC: 0.9823545375111459, ACC: :0.9441823899371069 Loss: 0.14998511970043182 \n",
      "[Test] number: 21, AUC: 0.97498166483315, ACC: :0.9389942291838417 Loss: 0.1901433765888214 \n",
      "[Test] number: 22, AUC: 0.983870916961826, ACC: :0.9378068739770867 Loss: 0.1540188044309616 \n",
      "[Test] number: 23, AUC: 0.9779460987996307, ACC: :0.9480249480249481 Loss: 0.15807223320007324 \n",
      "[Test] number: 24, AUC: 0.9839289755277536, ACC: :0.9421265141318977 Loss: 0.14759522676467896 \n",
      "[Test] number: 25, AUC: 0.9897493324145059, ACC: :0.9722753346080306 Loss: 0.09309477359056473 \n",
      "[Test] number: 26, AUC: 0.981961835466404, ACC: :0.9478827361563518 Loss: 0.13893727958202362 \n",
      "[Test] number: 27, AUC: 0.9699656886883866, ACC: :0.9442467378410438 Loss: 0.17597708106040955 \n",
      "[Test] number: 28, AUC: 0.9795588347368822, ACC: :0.9414784394250514 Loss: 0.1751454770565033 \n",
      "[Test] number: 29, AUC: 0.9834594961021353, ACC: :0.955767562879445 Loss: 0.13299918174743652 \n",
      "[Test] number: 30, AUC: 0.9749666577754068, ACC: :0.9478458049886621 Loss: 0.18127408623695374 \n",
      "[Test] number: 31, AUC: 0.974345100959562, ACC: :0.9430756159728122 Loss: 0.17775662243366241 \n",
      "[Test] number: 32, AUC: 0.9738736929957168, ACC: :0.941025641025641 Loss: 0.16722320020198822 \n",
      "[Test] number: 33, AUC: 0.979960050057759, ACC: :0.9430199430199431 Loss: 0.16500668227672577 \n",
      "[Test] number: 34, AUC: 0.9703897184596181, ACC: :0.9412416851441242 Loss: 0.17444247007369995 \n",
      "[Test] number: 35, AUC: 0.9758965961901843, ACC: :0.9337152209492635 Loss: 0.19423571228981018 \n",
      "[Test] number: 36, AUC: 0.9700325543541449, ACC: :0.9379785604900459 Loss: 0.18608738481998444 \n",
      "[Test] number: 37, AUC: 0.9819738167170192, ACC: :0.9564831261101243 Loss: 0.14171276986598969 \n",
      "[Test] number: 38, AUC: 0.9899849326678595, ACC: :0.9610516066212269 Loss: 0.11604352295398712 \n",
      "[Test] number: 39, AUC: 0.9754703389434097, ACC: :0.9436485195797517 Loss: 0.17075608670711517 \n",
      "[Test] number: 40, AUC: 0.9846479791395046, ACC: :0.9568874868559412 Loss: 0.12516233325004578 \n",
      "[Test] number: 41, AUC: 0.9748915638723771, ACC: :0.9413461538461538 Loss: 0.17930661141872406 \n",
      "[Test] number: 42, AUC: 0.9708047357584075, ACC: :0.935969868173258 Loss: 0.20845814049243927 \n",
      "[Test] number: 43, AUC: 0.9706119687404953, ACC: :0.9520089285714286 Loss: 0.18315789103507996 \n",
      "[Test] number: 44, AUC: 0.9861716550213786, ACC: :0.9581447963800905 Loss: 0.11230229586362839 \n",
      "[Test] number: 45, AUC: 0.9801107361615244, ACC: :0.9535714285714286 Loss: 0.15089832246303558 \n",
      "[Test] number: 46, AUC: 0.9794822752070247, ACC: :0.9382978723404255 Loss: 0.16997624933719635 \n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "\n",
    "if optimizer == \"sgd\":\n",
    "    opt = SGD(model.parameters(), learning_rate, momentum=0.9)\n",
    "elif optimizer == \"adam\":\n",
    "    opt = Adam(model.parameters(), learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.5)\n",
    "opt.lr_scheduler = lr_scheduler\n",
    "\n",
    "# 모델에서 미리 정의한 함수로 AUCS와 LOSS 계산    \n",
    "aucs, loss_means, accs, q_accs, q_cnts, Mv, w = \\\n",
    "    train_model(\n",
    "        model, test_loader, ckpts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.2845)\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(Mv[:, :-1])) # 컨셉 수 / 시퀀스? 임베딩 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0816e-03, 3.3317e-01, 3.7621e-04,  ..., 1.9917e-02,\n",
      "          1.3704e-03, 3.8838e-04],\n",
      "         [1.0816e-03, 3.3317e-01, 3.7621e-04,  ..., 1.9917e-02,\n",
      "          1.3704e-03, 3.8838e-04],\n",
      "         [1.0816e-03, 3.3317e-01, 3.7621e-04,  ..., 1.9917e-02,\n",
      "          1.3704e-03, 3.8838e-04],\n",
      "         ...,\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04]],\n",
      "\n",
      "        [[3.6447e-03, 9.1907e-04, 3.4935e-04,  ..., 2.1074e-04,\n",
      "          7.6429e-04, 1.4949e-03],\n",
      "         [3.6447e-03, 9.1907e-04, 3.4935e-04,  ..., 2.1074e-04,\n",
      "          7.6429e-04, 1.4949e-03],\n",
      "         [5.6119e-04, 4.3690e-03, 1.4877e-04,  ..., 4.8740e-01,\n",
      "          9.1777e-04, 6.7118e-04],\n",
      "         ...,\n",
      "         [1.0042e-03, 7.0250e-04, 1.5304e-03,  ..., 1.4714e-02,\n",
      "          5.3000e-03, 1.1293e-03],\n",
      "         [1.0042e-03, 7.0250e-04, 1.5304e-03,  ..., 1.4714e-02,\n",
      "          5.3000e-03, 1.1293e-03],\n",
      "         [1.4307e-02, 1.7040e-03, 8.9552e-03,  ..., 2.9552e-05,\n",
      "          1.1577e-02, 6.7982e-03]],\n",
      "\n",
      "        [[1.5382e-03, 4.9898e-03, 8.4102e-04,  ..., 3.3072e-03,\n",
      "          1.9009e-04, 8.4962e-04],\n",
      "         [1.5382e-03, 4.9898e-03, 8.4102e-04,  ..., 3.3072e-03,\n",
      "          1.9009e-04, 8.4962e-04],\n",
      "         [1.5382e-03, 4.9898e-03, 8.4102e-04,  ..., 3.3072e-03,\n",
      "          1.9009e-04, 8.4962e-04],\n",
      "         ...,\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [8.3362e-04, 2.3535e-03, 3.3315e-03,  ..., 4.4914e-04,\n",
      "          3.2363e-03, 4.4281e-03],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         ...,\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04]],\n",
      "\n",
      "        [[5.2902e-04, 4.2895e-05, 7.3862e-04,  ..., 9.5708e-02,\n",
      "          1.4630e-03, 7.5491e-04],\n",
      "         [8.3362e-04, 2.3535e-03, 3.3315e-03,  ..., 4.4914e-04,\n",
      "          3.2363e-03, 4.4281e-03],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         ...,\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04]],\n",
      "\n",
      "        [[2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [3.2732e-04, 1.7454e-03, 2.4558e-04,  ..., 1.0535e-03,\n",
      "          7.1374e-04, 3.2814e-04],\n",
      "         [8.3362e-04, 2.3535e-03, 3.3315e-03,  ..., 4.4914e-04,\n",
      "          3.2363e-03, 4.4281e-03],\n",
      "         ...,\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04],\n",
      "         [2.3979e-04, 3.9177e-04, 4.3399e-04,  ..., 5.6806e-06,\n",
      "          1.1918e-03, 4.2809e-04]]])\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0004)\n",
      "tensor(-1.8045)\n"
     ]
    }
   ],
   "source": [
    "# 모든 W값에 대한 빈 텐서를 생성\n",
    "# 마지막 layer 빼서 넘겨보기\n",
    "\n",
    "print(w[0][99][49])\n",
    "print(torch.min((w.unsqueeze(-1) * Mv[:, :-1]).sum(-2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
