{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import random \n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn import metrics \n",
    "\n",
    "from data_loaders.assist2009 import ASSIST2009\n",
    "from data_loaders.assist2012 import ASSIST2012\n",
    "from data_loaders.ednet01 import EdNet01\n",
    "\n",
    "from models.dkvmn_text import SUBJ_DKVMN\n",
    "from models.dkvmn_text import train_model as plus_train\n",
    "\n",
    "from models.utils import collate_fn, collate_ednet, cal_acc_class\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 1004\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dkvmn+'\n",
    "dataset_name = 'ASSIST2009'\n",
    "dataset = None\n",
    "ckpts = f\"ckpts/{model_name}/{dataset_name}/\"\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "    model_config = config[model_name]\n",
    "    train_config = config[\"train_config\"]\n",
    "    \n",
    "batch_size = train_config[\"batch_size\"]\n",
    "num_epochs = train_config[\"num_epochs\"]\n",
    "train_ratio = train_config[\"train_ratio\"]\n",
    "learning_rate = train_config[\"learning_rate\"]\n",
    "optimizer = train_config[\"optimizer\"] # can be sgd, adam\n",
    "seq_len = train_config[\"seq_len\"] # 샘플링 할 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 추가 가능\n",
    "collate_pt = collate_fn\n",
    "if dataset_name == \"ASSIST2009\":\n",
    "    dataset = ASSIST2009(seq_len, 'datasets/ASSIST2009/')\n",
    "elif dataset_name == \"ASSIST2012\":\n",
    "    dataset = ASSIST2012(seq_len, 'datasets/ASSIST2012/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, test_loader, ckpt_path):\n",
    "    '''\n",
    "        Args:\n",
    "            train_loader: the PyTorch DataLoader instance for training\n",
    "            test_loader: the PyTorch DataLoader instance for test\n",
    "            num_epochs: the number of epochs\n",
    "            opt: the optimization to train this model\n",
    "            ckpt_path: the path to save this model's parameters\n",
    "    '''\n",
    "    aucs = []\n",
    "    loss_means = []  \n",
    "    accs = []\n",
    "    q_accs = {}\n",
    "    \n",
    "    max_auc = 0\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load(os.path.join(ckpt_path, \"model.ckpt\"), map_location=device))\n",
    "    loss_mean = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            q, r, qshft_seqs, rshft_seqs, m, bert_s, bert_t, bert_m, q2diff_seqs, pid_seqs, pidshift, hint_seqs = data\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            y, Mv = model(q.long(), r.long(), bert_s, bert_t, bert_m, q2diff_seqs.long(), pid_seqs.long())\n",
    "\n",
    "            # y와 t 변수에 있는 행렬들에서 마스킹이 true로 된 값들만 불러옴\n",
    "            q = torch.masked_select(q, m).detach().cpu()\n",
    "            y = torch.masked_select(y, m).detach().cpu()\n",
    "            t = torch.masked_select(r, m).detach().cpu()\n",
    "\n",
    "            auc = metrics.roc_auc_score(\n",
    "                y_true=t.numpy(), y_score=y.numpy()\n",
    "            )\n",
    "            bin_y = [1 if p >= 0.5 else 0 for p in y.numpy()]\n",
    "            acc = metrics.accuracy_score(t.numpy(), bin_y)\n",
    "            loss = binary_cross_entropy(y, t) # 실제 y^T와 원핫 결합, 다음 answer 간 cross entropy\n",
    "\n",
    "            print(f\"[Test] number: {i}, AUC: {auc}, ACC: :{acc} Loss: {loss} \")\n",
    "\n",
    "            # evaluation metrics\n",
    "            aucs.append(auc)\n",
    "            loss_mean.append(loss)     \n",
    "            accs.append(acc)\n",
    "            q_accs, cnt = cal_acc_class(q.long(), t.long(), bin_y)\n",
    "        loss_means.append(np.mean(loss_mean))\n",
    "\n",
    "\n",
    "    return aucs, loss_means, accs, q_accs, cnt, Mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, **model_config)).to(device)\n",
    "train_model = train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할\n",
    "data_size = len(dataset)\n",
    "train_size = int(data_size * train_ratio) \n",
    "valid_size = int(data_size * ((1.0 - train_ratio) / 2.0))\n",
    "test_size = data_size - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, valid_size, test_size], generator=torch.Generator(device=device)\n",
    ")\n",
    "\n",
    "# pickle에 얼마만큼 분할했는지 읽기\n",
    "if os.path.exists(os.path.join(dataset.dataset_dir, \"train_indices.pkl\")):\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"train_indices.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        train_dataset.indices = pickle.load(f)\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"valid_indicies.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        valid_dataset.indices = pickle.load(f)\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"test_indices.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        test_dataset.indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "\n",
    "if optimizer == \"sgd\":\n",
    "    opt = SGD(model.parameters(), learning_rate, momentum=0.9)\n",
    "elif optimizer == \"adam\":\n",
    "    opt = Adam(model.parameters(), learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.5)\n",
    "opt.lr_scheduler = lr_scheduler\n",
    "\n",
    "# 모델에서 미리 정의한 함수로 AUCS와 LOSS 계산    \n",
    "aucs, loss_means, accs, q_accs, q_cnts, Mv = \\\n",
    "    train_model(\n",
    "        model, test_loader, ckpts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
