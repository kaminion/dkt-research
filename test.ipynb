{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import random \n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn import metrics \n",
    "\n",
    "from data_loaders.assist2009 import ASSIST2009\n",
    "from data_loaders.assist2012 import ASSIST2012\n",
    "from data_loaders.ednet01 import EdNet01\n",
    "\n",
    "from models.dkvmn_text import SUBJ_DKVMN\n",
    "from models.dkvmn_text import train_model as plus_train\n",
    "\n",
    "from models.utils import collate_fn, collate_ednet, cal_acc_class\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 1004\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dkvmn+'\n",
    "dataset_name = 'ASSIST2009'\n",
    "dataset = None\n",
    "ckpts = f\"ckpts/{model_name}/{dataset_name}/\"\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "    model_config = config[model_name]\n",
    "    train_config = config[\"train_config\"]\n",
    "    \n",
    "batch_size = train_config[\"batch_size\"]\n",
    "num_epochs = train_config[\"num_epochs\"]\n",
    "train_ratio = train_config[\"train_ratio\"]\n",
    "learning_rate = train_config[\"learning_rate\"]\n",
    "optimizer = train_config[\"optimizer\"] # can be sgd, adam\n",
    "seq_len = train_config[\"seq_len\"] # 샘플링 할 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 추가 가능\n",
    "collate_pt = collate_fn\n",
    "if dataset_name == \"ASSIST2009\":\n",
    "    dataset = ASSIST2009(seq_len, 'datasets/ASSIST2009/')\n",
    "elif dataset_name == \"ASSIST2012\":\n",
    "    dataset = ASSIST2012(seq_len, 'datasets/ASSIST2012/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, test_loader, ckpt_path):\n",
    "    '''\n",
    "        Args:\n",
    "            train_loader: the PyTorch DataLoader instance for training\n",
    "            test_loader: the PyTorch DataLoader instance for test\n",
    "            num_epochs: the number of epochs\n",
    "            opt: the optimization to train this model\n",
    "            ckpt_path: the path to save this model's parameters\n",
    "    '''\n",
    "    aucs = []\n",
    "    loss_means = []  \n",
    "    accs = []\n",
    "    q_accs = {}\n",
    "    \n",
    "    max_auc = 0\n",
    "    \n",
    "    # Test\n",
    "    model.load_state_dict(torch.load(os.path.join(ckpt_path, \"model.ckpt\"), map_location=device))\n",
    "    loss_mean = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            q, r, qshft_seqs, rshft_seqs, m, bert_s, bert_t, bert_m, q2diff_seqs, pid_seqs, pidshift, hint_seqs = data\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            y, Mv, w = model(q.long(), r.long(), bert_s, bert_t, bert_m, q2diff_seqs.long(), pid_seqs.long())\n",
    "\n",
    "            # y와 t 변수에 있는 행렬들에서 마스킹이 true로 된 값들만 불러옴\n",
    "            q = torch.masked_select(q, m).detach().cpu()\n",
    "            y = torch.masked_select(y, m).detach().cpu()\n",
    "            t = torch.masked_select(r, m).detach().cpu()\n",
    "\n",
    "            auc = metrics.roc_auc_score(\n",
    "                y_true=t.numpy(), y_score=y.numpy()\n",
    "            )\n",
    "            bin_y = [1 if p >= 0.5 else 0 for p in y.numpy()]\n",
    "            acc = metrics.accuracy_score(t.numpy(), bin_y)\n",
    "            loss = binary_cross_entropy(y, t) # 실제 y^T와 원핫 결합, 다음 answer 간 cross entropy\n",
    "\n",
    "            print(f\"[Test] number: {i}, AUC: {auc}, ACC: :{acc} Loss: {loss} \")\n",
    "\n",
    "            # evaluation metrics\n",
    "            aucs.append(auc)\n",
    "            loss_mean.append(loss)     \n",
    "            accs.append(acc)\n",
    "            q_accs, cnt = cal_acc_class(q.long(), t.long(), bin_y)\n",
    "        loss_means.append(np.mean(loss_mean))\n",
    "\n",
    "\n",
    "    return aucs, loss_means, accs, q_accs, cnt, Mv, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.DataParallel(SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, **model_config)).to(device)\n",
    "train_model = train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할\n",
    "data_size = len(dataset)\n",
    "train_size = int(data_size * train_ratio) \n",
    "valid_size = int(data_size * ((1.0 - train_ratio) / 2.0))\n",
    "test_size = data_size - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, valid_size, test_size], generator=torch.Generator(device=device)\n",
    ")\n",
    "\n",
    "# pickle에 얼마만큼 분할했는지 읽기\n",
    "if os.path.exists(os.path.join(dataset.dataset_dir, \"train_indices.pkl\")):\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"train_indices.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        train_dataset.indices = pickle.load(f)\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"valid_indicies.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        valid_dataset.indices = pickle.load(f)\n",
    "    with open(\n",
    "        os.path.join(dataset.dataset_dir, \"test_indices.pkl\"), \"rb\"\n",
    "    ) as f:\n",
    "        test_dataset.indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x00000120800403C8>\n",
      "[Test] number: 0, AUC: 0.9646951885545804, ACC: :0.9255774165953806 Loss: 0.19650167226791382 \n",
      "[Test] number: 1, AUC: 0.937318381042593, ACC: :0.9030303030303031 Loss: 0.2672674357891083 \n",
      "[Test] number: 2, AUC: 0.9574817518248175, ACC: :0.9142441860465116 Loss: 0.19510900974273682 \n",
      "[Test] number: 3, AUC: 0.9638503273134823, ACC: :0.9406028368794326 Loss: 0.16736353933811188 \n",
      "[Test] number: 4, AUC: 0.9740740274877779, ACC: :0.935813953488372 Loss: 0.17980289459228516 \n",
      "[Test] number: 5, AUC: 0.9332156147945622, ACC: :0.8936170212765957 Loss: 0.32247358560562134 \n",
      "[Test] number: 6, AUC: 0.9448115445778187, ACC: :0.905103668261563 Loss: 0.26523008942604065 \n",
      "[Test] number: 7, AUC: 0.9251589766341319, ACC: :0.9076433121019108 Loss: 0.28238198161125183 \n",
      "[Test] number: 8, AUC: 0.9464691393452423, ACC: :0.9108159392789373 Loss: 0.25684311985969543 \n",
      "[Test] number: 9, AUC: 0.9504729243321068, ACC: :0.8993288590604027 Loss: 0.2788538932800293 \n",
      "[Test] number: 10, AUC: 0.9709116510483458, ACC: :0.9277286135693216 Loss: 0.18746769428253174 \n",
      "[Test] number: 11, AUC: 0.9641453054369578, ACC: :0.9136069114470843 Loss: 0.21556968986988068 \n",
      "[Test] number: 12, AUC: 0.9455809854962014, ACC: :0.8974535443909153 Loss: 0.2668863534927368 \n",
      "[Test] number: 13, AUC: 0.9753586776859504, ACC: :0.9446366782006921 Loss: 0.16626349091529846 \n",
      "[Test] number: 14, AUC: 0.9590049672124223, ACC: :0.9148648648648648 Loss: 0.22584523260593414 \n",
      "[Test] number: 15, AUC: 0.9569156439554237, ACC: :0.9046624913013221 Loss: 0.23377887904644012 \n",
      "[Test] number: 16, AUC: 0.9461360888204903, ACC: :0.9029038112522686 Loss: 0.26893091201782227 \n",
      "[Test] number: 17, AUC: 0.9628405643126399, ACC: :0.9069965870307167 Loss: 0.22948400676250458 \n",
      "[Test] number: 18, AUC: 0.9631152746903779, ACC: :0.9105858170606372 Loss: 0.22532713413238525 \n",
      "[Test] number: 19, AUC: 0.9562803214069133, ACC: :0.9132791327913279 Loss: 0.2364482283592224 \n",
      "[Test] number: 20, AUC: 0.9626305069380038, ACC: :0.9072327044025157 Loss: 0.22301645576953888 \n",
      "[Test] number: 21, AUC: 0.9572362791834739, ACC: :0.9159109645507008 Loss: 0.2531948983669281 \n",
      "[Test] number: 22, AUC: 0.9637937819756003, ACC: :0.9124386252045826 Loss: 0.22714753448963165 \n",
      "[Test] number: 23, AUC: 0.9679911126500462, ACC: :0.9261954261954262 Loss: 0.19080640375614166 \n",
      "[Test] number: 24, AUC: 0.964992887854395, ACC: :0.9152086137281292 Loss: 0.22896899282932281 \n",
      "[Test] number: 25, AUC: 0.9690250464902991, ACC: :0.9397705544933078 Loss: 0.17992980778217316 \n",
      "[Test] number: 26, AUC: 0.9582801347526458, ACC: :0.9283387622149837 Loss: 0.20670871436595917 \n",
      "[Test] number: 27, AUC: 0.9512493447492574, ACC: :0.9098457888493475 Loss: 0.2363554686307907 \n",
      "[Test] number: 28, AUC: 0.960982452463514, ACC: :0.9127310061601642 Loss: 0.24442051351070404 \n",
      "[Test] number: 29, AUC: 0.9690694083531051, ACC: :0.9332176929748482 Loss: 0.1959419846534729 \n",
      "[Test] number: 30, AUC: 0.9513959278029698, ACC: :0.907785336356765 Loss: 0.2536815404891968 \n",
      "[Test] number: 31, AUC: 0.9588268832468408, ACC: :0.9286321155480034 Loss: 0.20986022055149078 \n",
      "[Test] number: 32, AUC: 0.9344234221466364, ACC: :0.9085470085470085 Loss: 0.2874242663383484 \n",
      "[Test] number: 33, AUC: 0.95986956103196, ACC: :0.9152421652421653 Loss: 0.23582397401332855 \n",
      "[Test] number: 34, AUC: 0.9606021780909674, ACC: :0.9135254988913526 Loss: 0.20948755741119385 \n",
      "[Test] number: 35, AUC: 0.9579104106021143, ACC: :0.897708674304419 Loss: 0.23761360347270966 \n",
      "[Test] number: 36, AUC: 0.9433292640390651, ACC: :0.9019908116385911 Loss: 0.27448585629463196 \n",
      "[Test] number: 37, AUC: 0.9628493796196013, ACC: :0.9209591474245116 Loss: 0.22688455879688263 \n",
      "[Test] number: 38, AUC: 0.9667718240888972, ACC: :0.9162609542356378 Loss: 0.21376702189445496 \n",
      "[Test] number: 39, AUC: 0.9612549232395619, ACC: :0.9274116523400191 Loss: 0.21444851160049438 \n",
      "[Test] number: 40, AUC: 0.9644690055707005, ACC: :0.9211356466876972 Loss: 0.19687242805957794 \n",
      "[Test] number: 41, AUC: 0.9576726740958389, ACC: :0.9086538461538461 Loss: 0.23830842971801758 \n",
      "[Test] number: 42, AUC: 0.9483442004118051, ACC: :0.8983050847457628 Loss: 0.2772110402584076 \n",
      "[Test] number: 43, AUC: 0.9609659658727646, ACC: :0.9185267857142857 Loss: 0.21900267899036407 \n",
      "[Test] number: 44, AUC: 0.9500955956477909, ACC: :0.9083710407239819 Loss: 0.23923614621162415 \n",
      "[Test] number: 45, AUC: 0.9545038849818512, ACC: :0.9035714285714286 Loss: 0.243881493806839 \n",
      "[Test] number: 46, AUC: 0.9635288747853681, ACC: :0.9085106382978724 Loss: 0.22968725860118866 \n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "\n",
    "if optimizer == \"sgd\":\n",
    "    opt = SGD(model.parameters(), learning_rate, momentum=0.9)\n",
    "elif optimizer == \"adam\":\n",
    "    opt = Adam(model.parameters(), learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma=0.5)\n",
    "opt.lr_scheduler = lr_scheduler\n",
    "\n",
    "# 모델에서 미리 정의한 함수로 AUCS와 LOSS 계산    \n",
    "print(iter(DataLoader(test_loader)))\n",
    "aucs, loss_means, accs, q_accs, q_cnts, Mv, w = \\\n",
    "    train_model(\n",
    "        model, test_loader, ckpts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9477)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(Mv[:, :-1])) # 컨셉 수 / 시퀀스? 임베딩 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.9650e-03, 2.6068e-04, 8.4568e-04,  ..., 1.0739e-03,\n",
      "          8.8093e-04, 3.0276e-04],\n",
      "         [1.9650e-03, 2.6068e-04, 8.4568e-04,  ..., 1.0739e-03,\n",
      "          8.8093e-04, 3.0276e-04],\n",
      "         [1.9650e-03, 2.6068e-04, 8.4568e-04,  ..., 1.0739e-03,\n",
      "          8.8093e-04, 3.0276e-04],\n",
      "         ...,\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04]],\n",
      "\n",
      "        [[3.5993e-03, 9.4661e-04, 6.0555e-04,  ..., 6.3319e-04,\n",
      "          2.9606e-04, 9.4043e-04],\n",
      "         [3.5993e-03, 9.4661e-04, 6.0555e-04,  ..., 6.3319e-04,\n",
      "          2.9606e-04, 9.4043e-04],\n",
      "         [1.0867e-03, 5.8854e-04, 1.8075e-03,  ..., 2.2399e-04,\n",
      "          1.9610e-04, 2.8893e-04],\n",
      "         ...,\n",
      "         [1.1919e-03, 1.1772e-03, 1.2758e-03,  ..., 1.8060e-03,\n",
      "          2.1791e-03, 1.5986e-03],\n",
      "         [1.1919e-03, 1.1772e-03, 1.2758e-03,  ..., 1.8060e-03,\n",
      "          2.1791e-03, 1.5986e-03],\n",
      "         [3.4360e-03, 9.3916e-04, 3.7155e-04,  ..., 2.5476e-03,\n",
      "          4.1165e-04, 6.2963e-04]],\n",
      "\n",
      "        [[5.8260e-05, 2.9190e-04, 3.9816e-04,  ..., 5.8283e-04,\n",
      "          4.2956e-04, 6.9498e-04],\n",
      "         [5.8260e-05, 2.9190e-04, 3.9816e-04,  ..., 5.8283e-04,\n",
      "          4.2956e-04, 6.9498e-04],\n",
      "         [5.8260e-05, 2.9190e-04, 3.9816e-04,  ..., 5.8283e-04,\n",
      "          4.2956e-04, 6.9498e-04],\n",
      "         ...,\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8191e-04, 1.0829e-04, 2.8837e-04,  ..., 8.1320e-04,\n",
      "          3.9088e-04, 4.3445e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         ...,\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04]],\n",
      "\n",
      "        [[4.2428e-04, 1.3079e-03, 3.5240e-04,  ..., 2.5544e-03,\n",
      "          4.6585e-03, 1.9361e-03],\n",
      "         [3.8191e-04, 1.0829e-04, 2.8837e-04,  ..., 8.1320e-04,\n",
      "          3.9088e-04, 4.3445e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         ...,\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04]],\n",
      "\n",
      "        [[3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.6777e-04, 1.2224e-04, 6.9433e-04,  ..., 1.4509e-03,\n",
      "          8.8369e-04, 5.4503e-04],\n",
      "         [3.8191e-04, 1.0829e-04, 2.8837e-04,  ..., 8.1320e-04,\n",
      "          3.9088e-04, 4.3445e-04],\n",
      "         ...,\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04],\n",
      "         [3.8164e-04, 4.8337e-04, 2.9430e-04,  ..., 2.5905e-04,\n",
      "          4.3908e-04, 2.0913e-04]]])\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 100, 100, 100])\n",
      "torch.Size([13, 100, 101])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-348763f24854>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSUBJ_DKVMN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_qid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_pid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 4"
     ]
    }
   ],
   "source": [
    "# 모든 W값에 대한 빈 텐서를 생성\n",
    "# 마지막 layer 빼서 넘겨보기\n",
    "\n",
    "# Read Process = w * M\n",
    "# 그래서 R 값은 마스크한 형태이고, W값은 마스킹 한 쪽에 0값 달라는 얘기인 것 같음\n",
    "# 그럼 결론적으로 메모리만 들어가면 됨\n",
    "# Memory \n",
    "\n",
    "a = torch.zeros_like(w)\n",
    "print(Mv[:, :-1].shape)\n",
    "a[12][99][4] = 1\n",
    "print(torch.concat([a.unsqueeze(-1), Mv[:, :-1]], dim=-1).sum(-2).shape)\n",
    "print(SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, **model_config).k_emb_layer(q))\n",
    "print(SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, **model_config).f_layer((torch.concat([a, torch.zeros_like(Mv[:, :-1])], dim=-1).sum(-2).T * torch.concat([a.unsqueeze(-1), Mv[:, :-1]], dim=-1).sum(-2))))\n",
    "\n",
    "\n",
    "print(w[0][99][4])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
