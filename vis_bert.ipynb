{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreting BERT Models with Captum library\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "from models.dkvmn_text import SUBJ_DKVMN\n",
    "\n",
    "from data_loaders.assist2009 import ASSIST2009\n",
    "from data_loaders.assist2012 import ASSIST2012\n",
    "from models.utils import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "model_name = \"dkvmn+\"\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "dataset_name = \"ASSIST2009\"\n",
    "ckpts = f\"ckpts/{model_name}/{dataset_name}/\"\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "    model_config = config[model_name]\n",
    "    train_config = config[\"train_config\"]\n",
    "    \n",
    "batch_size = train_config[\"batch_size\"]\n",
    "num_epochs = train_config[\"num_epochs\"]\n",
    "train_ratio = train_config[\"train_ratio\"]\n",
    "learning_rate = train_config[\"learning_rate\"]\n",
    "optimizer = train_config[\"optimizer\"] # can be sgd, adam\n",
    "seq_len = train_config[\"seq_len\"] # 샘플링 할 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, CharTensor, LongTensor\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "else:\n",
    "    from torch import FloatTensor, CharTensor, LongTensor\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def custom_collate(batch, pad_val=-1):\n",
    "    '''\n",
    "    This function for torch.utils.data.DataLoader\n",
    "\n",
    "    Returns:\n",
    "        q_seqs: the question(KC) sequences with the size of \\\n",
    "            [batch_size, maximum_sequence_length_in_the_batch]\n",
    "        r_seqs: the response sequences with the size of \\\n",
    "            [batch_size, maximum_sequence_length_in_the_batch]\n",
    "        qshft_seqs: the question(KC) sequences which were shifted \\\n",
    "            one step to the right with the size of \\\n",
    "            [batch_size, maximum_sequence_length_in_the_batch]\n",
    "        rshft_seqs: the response sequences which were shifted \\\n",
    "            one step to the right with the size of \\\n",
    "            [batch_size, maximum_sequence_length_in_the_batch]\n",
    "        mask_seqs: the mask sequences indicating where \\\n",
    "            the padded entry is with the size of \\\n",
    "            [batch_size, maximum_sequence_length_in_the_batch]\n",
    "    '''\n",
    "\n",
    "    q_seqs = []\n",
    "    r_seqs = []\n",
    "    qshft_seqs = []\n",
    "    rshft_seqs = []\n",
    "    at_seqs = []\n",
    "    atshft_seqs = []\n",
    "    q2diff_seqs = []\n",
    "    pid_seqs = []\n",
    "    pidshft_seqs = []\n",
    "    hint_seqs = []\n",
    "\n",
    "    # q_seq와 r_seq는 마지막 전까지만 가져옴 (마지막은 padding value)\n",
    "    # q_shft와 rshft는 처음 값 이후 가져옴 (우측 시프트 값이므로..)\n",
    "    for q_seq, r_seq, at_seq, q2diff, pid_seq, hint_seq in batch:\n",
    "        q_seqs.append(FloatTensor(q_seq[:-1])) \n",
    "        r_seqs.append(FloatTensor(r_seq[:-1]))\n",
    "        at_seqs.append(at_seq[:-1])\n",
    "        atshft_seqs.append(at_seq[1:])\n",
    "        qshft_seqs.append(FloatTensor(q_seq[1:]))\n",
    "        rshft_seqs.append(FloatTensor(r_seq[1:]))\n",
    "        q2diff_seqs.append(FloatTensor(q2diff[:-1]))\n",
    "        pid_seqs.append(FloatTensor(pid_seq[:-1]))\n",
    "        pidshft_seqs.append(FloatTensor(pid_seq[1:]))\n",
    "        hint_seqs.append(FloatTensor(hint_seq[:-1]))\n",
    "\n",
    "    # pad_sequence, 첫번째 인자는 sequence, 두번째는 batch_size가 첫 번째로 인자로 오게 하는 것이고, 3번째 인자의 경우 padding된 요소의 값\n",
    "    # 시퀀스 내 가장 길이가 긴 시퀀스를 기준으로 padding이 됨, 길이가 안맞는 부분은 늘려서 padding_value 값으로 채워줌\n",
    "    q_seqs = pad_sequence(\n",
    "        q_seqs, batch_first=True, padding_value=pad_val\n",
    "    )\n",
    "    r_seqs = pad_sequence(\n",
    "        r_seqs, batch_first=True, padding_value=pad_val\n",
    "    )\n",
    "    q2diff_seqs = pad_sequence(\n",
    "        q2diff_seqs, batch_first=True, padding_value=pad_val\n",
    "    )\n",
    "    qshft_seqs = pad_sequence(\n",
    "        qshft_seqs, batch_first=True, padding_value=pad_val\n",
    "    )\n",
    "    rshft_seqs = pad_sequence(\n",
    "        rshft_seqs, batch_first=True, padding_value=pad_val\n",
    "    )\n",
    "    pid_seqs = pad_sequence(\n",
    "        pid_seqs, batch_first=True, padding_value=pad_val\n",
    "    )\n",
    "    pidshft_seqs = pad_sequence(\n",
    "        pidshft_seqs, batch_first=True, padding_value=pad_val\n",
    "    )\n",
    "    hint_seqs = pad_sequence(\n",
    "        hint_seqs, batch_first=True, padding_value=pad_val\n",
    "    )\n",
    "\n",
    "    # 마스킹 시퀀스 생성 \n",
    "    # 일반 question 시퀀스: 패딩 밸류와 다른 값들은 모두 1로 처리, 패딩 처리된 값들은 0으로 처리.\n",
    "    # 일반 question padding 시퀀스: 한 칸 옆으로 시프팅 된 시퀀스 값들이 패딩 값과 다를 경우 1로 처리, 패딩 처리 된 값들은 0으로 처리.\n",
    "    # 마스킹 시퀀스: 패딩 처리 된 시퀀스 밸류들은 모두 0, 두 값 모두 패딩처리 되지 않았을 경우 1로 처리. (원본 시퀀스와 shift 시퀀스 모두의 값)\n",
    "    # 예를 들어, 현재 값과 다음 값이 패딩 값이 아닐 경우 1, 현재 값과 다음 값 둘 중 하나라도 패딩일 경우 0으로 처리함.\n",
    "    mask_seqs = (q_seqs != pad_val) * (qshft_seqs != pad_val)\n",
    "\n",
    "    # 원본 값의 다음 값이(shift value) 패딩이기만 해도 마스킹 시퀀스에 의해 값이 0로 변함. 아닐경우 원본 시퀀스 데이터를 가짐.\n",
    "    q_seqs, r_seqs, qshft_seqs, rshft_seqs, q2diff_seqs, pid_seqs, pidshft_seqs, hint_seqs = \\\n",
    "        q_seqs * mask_seqs, r_seqs * mask_seqs, qshft_seqs * mask_seqs, \\\n",
    "        rshft_seqs * mask_seqs, q2diff_seqs * mask_seqs, pid_seqs * mask_seqs, \\\n",
    "        pidshft_seqs * mask_seqs, hint_seqs * mask_seqs\n",
    "    \n",
    "\n",
    "    # Word2vec\n",
    "\n",
    "    # BERT preprocessing\n",
    "    bert_details = []\n",
    "\n",
    "    # def mapmax(data):\n",
    "    #     return max(data, key=len)\n",
    "\n",
    "    # 2차원에서 가장 긴 문장 추출\n",
    "    # SENT_LEN = len(max(map(mapmax, at_seqs), key=len))\n",
    "\n",
    "    for answer_text in at_seqs:\n",
    "        text = list(map(str, answer_text))\n",
    "        # print(f\"============= text: {text} ================\")\n",
    "        \n",
    "        encoded_bert_sent = tokenizer.encode_plus(\n",
    "            text, add_special_tokens=False, truncation=True, return_token_type_ids=False\n",
    "        )\n",
    "        bert_details.append(encoded_bert_sent)\n",
    "    \n",
    "    # 정답지 추가\n",
    "    # proc_atshft_seqs = []\n",
    "    # # SENT_LEN = q_seqs.size(0)\n",
    "    # for answer_text in atshft_seqs:\n",
    "    #     text = \" \".join(map(str, answer_text))\n",
    "    #     encoded_bert_sent = bert_tokenizer.encode_plus(\n",
    "    #         text, add_special_tokens=True, padding='max_length', truncation=True\n",
    "    #     )\n",
    "    #     proc_atshft_seqs.append(encoded_bert_sent)\n",
    "\n",
    "    bert_sentences = LongTensor([text[\"input_ids\"] for text in bert_details])\n",
    "    # bert_sentence_types = LongTensor([text[\"token_type_ids\"] for text in bert_details])\n",
    "    bert_sentence_att_mask = LongTensor([text[\"attention_mask\"] for text in bert_details])\n",
    "    # proc_atshft_sentences = LongTensor([text[\"input_ids\"] for text in proc_atshft_seqs])\n",
    "\n",
    "    return q_seqs, r_seqs, qshft_seqs, rshft_seqs, mask_seqs, bert_sentences, [], bert_sentence_att_mask, q2diff_seqs, pid_seqs, pidshft_seqs, at_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# 데이터셋 추가 가능\n",
    "collate_pt = custom_collate\n",
    "dataset = ASSIST2009(seq_len, 'datasets/ASSIST2009/')\n",
    "if dataset_name == \"ASSIST2012\":\n",
    "    dataset = ASSIST2012(seq_len, 'datasets/ASSIST2012/')\n",
    "    \n",
    "# 데이터셋 분할\n",
    "data_size = len(dataset)\n",
    "train_size = int(data_size * train_ratio) \n",
    "valid_size = int(data_size * ((1.0 - train_ratio) / 2.0))\n",
    "test_size = data_size - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, valid_size, test_size], generator=torch.Generator(device=device)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True,\n",
    "    collate_fn=collate_pt, generator=torch.Generator(device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dim_s': 200, 'size_m': 20}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define help function\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaminion/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# load model / tokenizer in collate function\n",
    "model = torch.nn.DataParallel(SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, **model_config)).to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(ckpts, \"model.ckpt\"), map_location=device))\n",
    "model.eval()\n",
    "model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict answer (testing model)\n",
    "def predict(q, r, at_s, at_t, at_m):\n",
    "    output = model(q, r, at_s, at_t, at_m)\n",
    "    return output[0]\n",
    "\n",
    "# custom forward function\n",
    "def custom_forward(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs, position_ids, token_type_ids, attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "cls_token_id = tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper function for constructing references / baseline for word tokens\n",
    "# 이게 원래 인풋으로 들어가는 Question과 아웃풋의 Answering이 존재했는데 나는 Answering 안써서 Answering 제거함\n",
    "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False, max_length=510, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
    "    \n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "    \n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n",
    "\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device) # * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with 'torch.randperm(seq_length, device=device)'\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "    \n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "\n",
    "# 어텐션 마스크 구성\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "\n",
    "# 버트 임베딩 구성\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.module.bertmodel.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.module.bertmodel.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings\n",
    "\n",
    "\n",
    "# 시퀀스에서 각 워드 토큰에 대해 속성을 요약해주는 헬퍼 함수\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 torch.Size([15, 200])\n",
      "['26/35' '166' '-7,7' '118' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      " ' ' ' ' ' ' ' ' ' ']\n",
      "200\n",
      "[]\n",
      "torch.Size([1, 512]) torch.Size([1, 512]) torch.Size([1, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[15, 1, 1, 512]' is invalid for input of size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(input_ids\u001b[39m.\u001b[39mshape, attention_mask\u001b[39m.\u001b[39mshape, ref_input_ids\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     21\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mrepeat(\u001b[39m15\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m score \u001b[39m=\u001b[39m predict(q\u001b[39m.\u001b[39;49mlong(), r\u001b[39m.\u001b[39;49mlong(), input_ids, token_type_id, attention_mask)\n\u001b[1;32m     24\u001b[0m attributions, delta \u001b[39m=\u001b[39m lig\u001b[39m.\u001b[39mattribute(inputs\u001b[39m=\u001b[39mq\u001b[39m.\u001b[39mlong(),\n\u001b[1;32m     25\u001b[0m     baselines\u001b[39m=\u001b[39mref_input_ids,\n\u001b[1;32m     26\u001b[0m     internal_batch_size\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m,\n\u001b[1;32m     27\u001b[0m     additional_forward_args\u001b[39m=\u001b[39m(r\u001b[39m.\u001b[39mlong(), input_ids, token_type_id, attention_mask),\n\u001b[1;32m     28\u001b[0m     return_convergence_delta\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m attributions_sum \u001b[39m=\u001b[39m summarize_attributions(attributions)\n",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(q, r, at_s, at_t, at_m)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(q, r, at_s, at_t, at_m):\n\u001b[0;32m----> 3\u001b[0m     output \u001b[39m=\u001b[39m model(q, r, at_s, at_t, at_m)\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m output[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39m\"\u001b[39m\u001b[39mDataParallel.forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    169\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m         \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/models/dkvmn_text.py:97\u001b[0m, in \u001b[0;36mSUBJ_DKVMN.forward\u001b[0;34m(self, q, r, at_s, at_t, at_m)\u001b[0m\n\u001b[1;32m     94\u001b[0m batch_size \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     96\u001b[0m \u001b[39m# BERT를 사용하지 않는다면 주석처리\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m em_at \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mat_emb_layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbertmodel(input_ids\u001b[39m=\u001b[39;49mat_s,\n\u001b[1;32m     98\u001b[0m                attention_mask\u001b[39m=\u001b[39;49mat_m,\n\u001b[1;32m     99\u001b[0m             \u001b[39m#    token_type_ids=at_t\u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m                )\u001b[39m.\u001b[39mlast_hidden_state)\n\u001b[1;32m    101\u001b[0m em_at \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mat2_emb_layer(em_at\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    103\u001b[0m \u001b[39m# unsqueeze는 지정된 위치에 크기가 1인 텐서 생성 \u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m# repeat은 현재 갖고 있는 사이즈에 매개변수 만큼 곱해주는 것 (공간 생성, element가 있다면 해당 element 곱해줌.)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m# 그래서 Mv0 맨 앞에 차원 하나 추가하고 추가한 차원에 batch_size 곱해줌. 나머지는 메모리사이즈 * 1, 임베딩 공간 * 1 해줌.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:609\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    605\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    607\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    610\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[1;32m    611\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    612\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    613\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    614\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    615\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    616\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:375\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    368\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    369\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    370\u001b[0m         hidden_state,\n\u001b[1;32m    371\u001b[0m         attn_mask,\n\u001b[1;32m    372\u001b[0m         head_mask[i],\n\u001b[1;32m    373\u001b[0m     )\n\u001b[1;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    376\u001b[0m         hidden_state,\n\u001b[1;32m    377\u001b[0m         attn_mask,\n\u001b[1;32m    378\u001b[0m         head_mask[i],\n\u001b[1;32m    379\u001b[0m         output_attentions,\n\u001b[1;32m    380\u001b[0m     )\n\u001b[1;32m    382\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    296\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    297\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    298\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    299\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    302\u001b[0m )\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:221\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    219\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    222\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(\n\u001b[1;32m    223\u001b[0m     mask, torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mfinfo(scores\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin)\n\u001b[1;32m    224\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    226\u001b[0m weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[15, 1, 1, 512]' is invalid for input of size 512"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        q, r, qshft_seqs, rshft_seqs, mask_seqs, oinput_ids, otoken_type_ids, oattention_mask, q2diff_seqs, pid_seqs, pidshft_seqs, at_seqs \\\n",
    "            = data\n",
    "        print(len(at_seqs), oinput_ids.shape)\n",
    "        for answer_text in at_seqs:\n",
    "            text = []\n",
    "            print(answer_text)\n",
    "            # answer_text = answer_text.tolist()\n",
    "            print(len(answer_text))\n",
    "            answer_text = list(map(str, answer_text))          \n",
    "            print(text)\n",
    "            input_ids, ref_input_ids, sep_id = construct_input_ref_pair(answer_text, ref_token_id, sep_token_id, cls_token_id)\n",
    "            token_type_id, ref_token_type_id = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "            position_id, ref_position_id = construct_input_ref_pos_id_pair(input_ids)\n",
    "            attention_mask = construct_attention_mask(input_ids)\n",
    "            \n",
    "            lig = LayerIntegratedGradients(model, model.module.bertmodel.embeddings)\n",
    "\n",
    "            print(input_ids.shape, attention_mask.shape, ref_input_ids.shape)\n",
    "            input_ids = input_ids.repeat(15, 1)\n",
    "            score = predict(q.long(), r.long(), input_ids, token_type_id, attention_mask)\n",
    "\n",
    "            attributions, delta = lig.attribute(inputs=q.long(),\n",
    "                baselines=ref_input_ids,\n",
    "                internal_batch_size=15,\n",
    "                additional_forward_args=(r.long(), input_ids, token_type_id, attention_mask),\n",
    "                return_convergence_delta=True)\n",
    "            attributions_sum = summarize_attributions(attributions)\n",
    "            # attributions_end_sum = summarize_attributions(attributions_end)\n",
    "            indices = input_ids[0].detach().tolist()\n",
    "            all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "            end_position_vis = viz.VisualizationDataRecord(\n",
    "                            attributions_sum,\n",
    "                            torch.max(torch.softmax(score, dim=0)),\n",
    "                            torch.argmax(score),\n",
    "                            torch.argmax(score),\n",
    "                            str(indices.index(input_ids)),\n",
    "                            attributions_sum.sum(),       \n",
    "                            all_tokens,\n",
    "                            delta)\n",
    "            viz.visualize_text([end_position_vis])\n",
    "            break\n",
    "\n",
    "            \n",
    "\n",
    "        # 워드 임베딩 변화량 볼 수 있음\n",
    "        # attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "        #                         baselines=ref_input_ids,\n",
    "        #                         additional_forward_args=(token_type_ids, position_ids, attention_mask, 0),\n",
    "        #                         return_convergence_delta=True)\n",
    "        # attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "        #                         additional_forward_args=(token_type_ids, position_ids, attention_mask, 1),\n",
    "        #                         return_convergence_delta=True)\n",
    "        \n",
    "\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([92., 92., 69., 69., 92., 92., 69., 92., 69., 92., 39., 92., 39., 69.,\n",
      "        92., 69., 92., 39., 92., 92., 39., 92., 39., 92., 39., 69., 92., 92.,\n",
      "        39., 69., 92., 20., 22., 20., 20., 20., 20., 20., 20., 63., 63., 63.,\n",
      "        63., 63., 63., 63., 63., 63., 63., 63., 63., 63., 63., 60., 60., 60.,\n",
      "        60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60.,\n",
      "        60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60., 60.,\n",
      "         0., 89., 89.,  0.,  1.,  0.,  0.,  1.,  0., 89.,  1.,  0.,  0.,  1.,\n",
      "         0., 89.,  1.,  0.,  0., 89.,  0., 89., 89.,  0.,  1.,  0., -0., -0.,\n",
      "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "        -0., -0., -0., -0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3000x200 and 512x200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(q[\u001b[39m0\u001b[39m])\n\u001b[1;32m      7\u001b[0m all_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(indices)\n\u001b[0;32m----> 8\u001b[0m score \u001b[39m=\u001b[39m predict(q\u001b[39m.\u001b[39;49mlong(), r\u001b[39m.\u001b[39;49mlong(), input_ids, token_type_ids, attention_mask)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39margmax(score), input_ids\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m attr \u001b[39m=\u001b[39m LayerIntegratedGradients(predict, model\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbertmodel\u001b[39m.\u001b[39membeddings)\n",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(q, r, at_s, at_t, at_m)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(q, r, at_s, at_t, at_m):\n\u001b[0;32m----> 3\u001b[0m     output \u001b[39m=\u001b[39m model(q, r, at_s, at_t, at_m)\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m output[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:167\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39m\"\u001b[39m\u001b[39mDataParallel.forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    169\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[1;32m    170\u001b[0m         \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/models/dkvmn_text.py:101\u001b[0m, in \u001b[0;36mSUBJ_DKVMN.forward\u001b[0;34m(self, q, r, at_s, at_t, at_m)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m# BERT를 사용하지 않는다면 주석처리\u001b[39;00m\n\u001b[1;32m     97\u001b[0m em_at \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mat_emb_layer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbertmodel(input_ids\u001b[39m=\u001b[39mat_s,\n\u001b[1;32m     98\u001b[0m                attention_mask\u001b[39m=\u001b[39mat_m,\n\u001b[1;32m     99\u001b[0m             \u001b[39m#    token_type_ids=at_t\u001b[39;00m\n\u001b[1;32m    100\u001b[0m                )\u001b[39m.\u001b[39mlast_hidden_state)\n\u001b[0;32m--> 101\u001b[0m em_at \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mat2_emb_layer(em_at\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m    103\u001b[0m \u001b[39m# unsqueeze는 지정된 위치에 크기가 1인 텐서 생성 \u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m# repeat은 현재 갖고 있는 사이즈에 매개변수 만큼 곱해주는 것 (공간 생성, element가 있다면 해당 element 곱해줌.)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m# 그래서 Mv0 맨 앞에 차원 하나 추가하고 추가한 차원에 batch_size 곱해줌. 나머지는 메모리사이즈 * 1, 임베딩 공간 * 1 해줌.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m Mvt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMv0\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(batch_size, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m# parameter로 메모리 사이즈만큼, 그리고 임베딩 공간만큼 구해줬음\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/workspace/proj/dkt-research/env_research/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3000x200 and 512x200)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        q, r, qshft_seqs, rshft_seqs, mask_seqs, input_ids, token_type_ids, attention_mask, q2diff_seqs, pid_seqs, pidshft_seqs, at_seqs \\\n",
    "            = data\n",
    "        indices = input_ids[0].detach().tolist()\n",
    "        print(q[0])\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "        score = predict(q.long(), r.long(), input_ids, token_type_ids, attention_mask)\n",
    "        print(torch.argmax(score), input_ids.shape)\n",
    "        attr = LayerIntegratedGradients(predict, model.module.bertmodel.embeddings)\n",
    "        attributions, delta = attr.attribute((q.long()), additional_forward_args=(r.long(), input_ids, token_type_ids, attention_mask), \\\n",
    "                                      internal_batch_size=15, target=2, return_convergence_delta=True, n_steps=10)\n",
    "        # Assuming attributions is a tensor\n",
    "        print(attributions[0])\n",
    "        attributions_sum = summarize_attributions(attributions[0])\n",
    "        # attributions_end_sum = summarize_attributions(attributions_end)\n",
    "\n",
    "        end_position_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_sum,\n",
    "                        torch.max(torch.softmax(score, dim=0)),\n",
    "                        torch.argmax(score),\n",
    "                        torch.argmax(score),\n",
    "                        str(torch.argmax(score)),\n",
    "                        attributions_sum.sum(),       \n",
    "                        all_tokens,\n",
    "                        delta)\n",
    "        print(f\"end_position_vis: {indices}\")\n",
    "        viz.visualize_text([end_position_vis])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 5 at dim 2 (got 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32m<ipython-input-90-b9ca63e96671>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     30\u001b[0m         \u001b[1;31m# print(input_ids)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 31\u001b[1;33m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     33\u001b[0m         \u001b[0mref_input_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mref_input_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 5 at dim 2 (got 15)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        q, r, qshft_seqs, rshft_seqs, mask_seqs, input_ids, token_type_ids, attention_mask, q2diff_seqs, pid_seqs, pidshft_seqs, at_seqs \\\n",
    "            = data\n",
    "        \n",
    "        input_ids = []\n",
    "        ref_input_ids = []\n",
    "        token_type_ids = []\n",
    "        ref_token_type_ids = []\n",
    "        position_ids = []\n",
    "        ref_position_ids = []\n",
    "        attention_masks = []\n",
    "        \n",
    "                \n",
    "        for answer_text in at_seqs:\n",
    "            text = ' '.join(map(str, answer_text))\n",
    "            # print(f\"============= text: {text} ================\")\n",
    "            \n",
    "            input_id, ref_input_id, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
    "            token_type_id, ref_token_type_id = construct_input_ref_token_type_pair(input_id, sep_id)\n",
    "            position_id, ref_position_id = construct_input_ref_pos_id_pair(input_id)\n",
    "            attention_mask = construct_attention_mask(input_id)\n",
    "            \n",
    "            input_ids.append(input_id)\n",
    "            ref_input_ids.append(ref_input_id)\n",
    "            token_type_ids.append(token_type_id)\n",
    "            ref_token_type_ids.append(ref_token_type_id)\n",
    "            attention_masks.append(attention_mask)\n",
    "            \n",
    "        # print(input_ids)\n",
    "        input_ids = torch.tensor([t.numpy() for t in input_ids])\n",
    "        print(input_ids)\n",
    "        ref_input_ids = [t.numpy() for t in ref_input_ids]\n",
    "        token_type_ids = [t.numpy() for t in token_type_ids]\n",
    "        ref_token_type_ids = [t.numpy() for t in ref_token_type_ids]\n",
    "        position_ids = [t.numpy() for t in position_ids]\n",
    "        ref_position_ids = [t.numpy() for t in ref_position_ids]\n",
    "        attention_masks = [t.numpy() for t in attention_masks]\n",
    "            \n",
    "        score = predict(q.long(), r.long(), input_ids, token_type_ids, attention_mask)\n",
    "            \n",
    "        indices = input_ids[0].detach().tolist()\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "        # 워드 임베딩 변화량 볼 수 있음\n",
    "        lig = LayerIntegratedGradients(model, model.module.bertmodel.embeddings)\n",
    "        # attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "        #                         baselines=ref_input_ids,\n",
    "        #                         additional_forward_args=(token_type_ids, position_ids, attention_mask, 0),\n",
    "        #                         return_convergence_delta=True)\n",
    "        # attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "        #                         additional_forward_args=(token_type_ids, position_ids, attention_mask, 1),\n",
    "        #                         return_convergence_delta=True)\n",
    "        \n",
    "        attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                        baselines=ref_input_ids,\n",
    "                        n_steps=700,\n",
    "                        internal_batch_size=3,\n",
    "                        return_convergence_delta=True)\n",
    "        attributions_sum = summarize_attributions(attributions)\n",
    "        # attributions_end_sum = summarize_attributions(attributions_end)\n",
    "        \n",
    "        end_position_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_sum,\n",
    "                        torch.max(torch.softmax(score, dim=0)),\n",
    "                        torch.argmax(score),\n",
    "                        torch.argmax(score),\n",
    "                        str(indices.index(input_ids)),\n",
    "                        attributions_sum.sum(),       \n",
    "                        all_tokens,\n",
    "                        delta)\n",
    "        viz.visualize_text([end_position_vis])\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # print(f\"Affected Text: {all_tokens[torch.argmax(torch.tensor(score[0])) : torch.argmax(torch.tensor(score[-1])) + 1]}\")\n",
    "        \n",
    "\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
