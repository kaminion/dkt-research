{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import os\n",
    "from models.dkvmn_text import SUBJ_DKVMN\n",
    "from data_loaders.assist2009 import ASSIST2009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1638660/2568725576.py:5: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_full = pd.read_csv('./FeatureImportanceDL/dataset/skill_builder_data.csv', encoding='ISO-8859-1').drop(labels=[\"bottom_hint\", \"answer_id\"], axis=1).dropna()\n"
     ]
    }
   ],
   "source": [
    "dataset_label = \"correct\"\n",
    "seq_len = 100\n",
    "\n",
    "# 데이터 수정\n",
    "df_full = pd.read_csv('./FeatureImportanceDL/dataset/skill_builder_data.csv', encoding='ISO-8859-1').drop(labels=[\"bottom_hint\", \"answer_id\"], axis=1).dropna()\n",
    "all_X = df_full.drop(labels=dataset_label, axis=1)\n",
    "\n",
    "pre_columns = ['type', 'answer_type', 'skill_name', 'tutor_mode', 'answer_text']\n",
    "encoded_X = all_X.copy()\n",
    "all_X[pre_columns] = encoded_X[pre_columns].apply(lambda x: preprocessing.LabelEncoder().fit_transform(x))\n",
    "\n",
    "all_y = df_full[dataset_label]\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "ckpt_path = os.path.join(\"ckpts\", \"dkvmn+\")\n",
    "ckpt_path = os.path.join(ckpt_path, \"ASSIST2009\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "estimator should be an estimator implementing 'fit' method, DataParallel(\n  (module): SUBJ_DKVMN(\n    (k_emb_layer): Embedding(103, 100)\n    (d_emb_layer): Embedding(103, 100)\n    (transformer_encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0): TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n          )\n          (linear1): Linear(in_features=100, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=100, bias=True)\n          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (bertmodel): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (at_emb_layer): Linear(in_features=768, out_features=100, bias=True)\n    (at2_emb_layer): Linear(in_features=512, out_features=100, bias=True)\n    (qr_emb_layer): Embedding(1434172, 100)\n    (v_emb_layer): Linear(in_features=200, out_features=100, bias=True)\n    (e_layer): Linear(in_features=100, out_features=100, bias=True)\n    (a_layer): Linear(in_features=100, out_features=100, bias=True)\n    (f_layer): Linear(in_features=200, out_features=100, bias=True)\n    (p_layer): Linear(in_features=100, out_features=1, bias=True)\n    (dropout_layer): Dropout(p=0.2, inplace=False)\n  )\n) was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDataParallel(SUBJ_DKVMN(dataset\u001b[39m.\u001b[39mnum_q, num_qid\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39mnum_pid, \n\u001b[1;32m     18\u001b[0m                                          dim_s\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, size_m\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ckpt_path, \u001b[39m\"\u001b[39m\u001b[39mmodel.ckpt\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m---> 21\u001b[0m result \u001b[39m=\u001b[39m permutation_importance(estimator\u001b[39m=\u001b[39;49mmodel, X\u001b[39m=\u001b[39;49mX_tr, y\u001b[39m=\u001b[39;49my_tr, scoring\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mf1\u001b[39;49m\u001b[39m\"\u001b[39;49m, n_repeats\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/dkt-research/env_research/lib/python3.11/site-packages/sklearn/inspection/_permutation_importance.py:251\u001b[0m, in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[1;32m    249\u001b[0m     scorer \u001b[39m=\u001b[39m scoring\n\u001b[1;32m    250\u001b[0m \u001b[39melif\u001b[39;00m scoring \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(scoring, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 251\u001b[0m     scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39;49mscoring)\n\u001b[1;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     scorers_dict \u001b[39m=\u001b[39m _check_multimetric_scoring(estimator, scoring)\n",
      "File \u001b[0;32m~/dkt-research/env_research/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:474\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Determine scorer from user options.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m \u001b[39mA TypeError will be thrown if the estimator cannot be scored.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m    ``scorer(estimator, X, y)``.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(estimator, \u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 474\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mestimator should be an estimator implementing \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method, \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m was passed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m         \u001b[39m%\u001b[39m estimator\n\u001b[1;32m    477\u001b[0m     )\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scoring, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m get_scorer(scoring)\n",
      "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, DataParallel(\n  (module): SUBJ_DKVMN(\n    (k_emb_layer): Embedding(103, 100)\n    (d_emb_layer): Embedding(103, 100)\n    (transformer_encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0): TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n          )\n          (linear1): Linear(in_features=100, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=100, bias=True)\n          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (bertmodel): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (at_emb_layer): Linear(in_features=768, out_features=100, bias=True)\n    (at2_emb_layer): Linear(in_features=512, out_features=100, bias=True)\n    (qr_emb_layer): Embedding(1434172, 100)\n    (v_emb_layer): Linear(in_features=200, out_features=100, bias=True)\n    (e_layer): Linear(in_features=100, out_features=100, bias=True)\n    (a_layer): Linear(in_features=100, out_features=100, bias=True)\n    (f_layer): Linear(in_features=200, out_features=100, bias=True)\n    (p_layer): Linear(in_features=100, out_features=1, bias=True)\n    (dropout_layer): Dropout(p=0.2, inplace=False)\n  )\n) was passed"
     ]
    }
   ],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    all_X,\n",
    "    all_y,\n",
    "    test_size = 0.3,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    all_X,\n",
    "    all_y,\n",
    "    test_size = 0.3,\n",
    "    random_state = 69\n",
    ")\n",
    "\n",
    "dataset = ASSIST2009(seq_len)\n",
    "\n",
    "model = torch.nn.DataParallel(SUBJ_DKVMN(dataset.num_q, num_qid=dataset.num_pid, \n",
    "                                         dim_s=100, size_m=50)).to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(ckpt_path, \"model.ckpt\")))\n",
    "\n",
    "result = permutation_importance(estimator=model, X=X_tr, y=y_tr, scoring=\"f1\", n_repeats=5, random_state=42, n_jobs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_fi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
